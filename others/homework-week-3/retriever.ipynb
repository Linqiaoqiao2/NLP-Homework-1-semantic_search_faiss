{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework Week 3\n",
    "Tasks:\n",
    "1. Design a Retriever class\n",
    "• Methods: add_documents(), query(), save(), load()\n",
    "• Let it handle chunking + indexing internally\n",
    "• Use FAISS and SentenceTransformers under the hood\n",
    "2. Feed Real Documents\n",
    "• Try .txt, .md, or .pdf files (if ambitious)\n",
    "• Preprocess and chunk them (include a chunking function)\n",
    "3. Write Tests\n",
    "• Given a document and a query, does your retriever return the expected chunk?\n",
    "4. Document Your Code\n",
    "• Add docstrings, usage examples, and push to GitHub\n",
    "\n",
    "Deliverables:\n",
    "1. A retriever.py module with reusable class\n",
    "2. One or more loaded document sources\n",
    "3. Working local search using queries\n",
    "4. Committed README update and usage instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /home/codespace/.python/current/lib/python3.12/site-packages (4.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: tqdm in /home/codespace/.python/current/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (2.6.0+cpu)\n",
      "Requirement already satisfied: scikit-learn in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (76.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/codespace/.python/current/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/codespace/.python/current/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/codespace/.python/current/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /home/codespace/.python/current/lib/python3.12/site-packages (1.11.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /home/codespace/.local/lib/python3.12/site-packages (from faiss-cpu) (2.2.4)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from faiss-cpu) (24.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyMuPDF in /home/codespace/.python/current/lib/python3.12/site-packages (1.25.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentence-transformers\n",
    "%pip install faiss-cpu\n",
    "%pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import fitz  # PyMuPDF\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Retriever:\n",
    "    \"\"\"\n",
    "    Retriever class for document indexing and semantic search using FAISS and SentenceTransformers.\n",
    "    \"\"\"\n",
    "    # Initialize Retriever with embedding model and chunk parameters, so they can be reused across methods.\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\", chunk_size: int = 200, chunk_overlap: int = 30):\n",
    "        \n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.documents: List[str] = []\n",
    "        self.index = None\n",
    "    \n",
    "    \"\"\"\n",
    "    Load document from file path and return text content(String).\n",
    "    Supports .txt, .md, and .pdf formats.\n",
    "    For .pdf, uses PyMuPDF to extract text.\n",
    "    For .txt and .md, reads the file directly.\n",
    "    Raises ValueError for unsupported file formats.\n",
    "    \"\"\"\n",
    "    def load_document(self, file_path: str) -> str:\n",
    "        \n",
    "        suffix = Path(file_path).suffix.lower()\n",
    "        if suffix in (\".txt\", \".md\"):\n",
    "            return Path(file_path).read_text(encoding=\"utf-8\")\n",
    "        elif suffix == \".pdf\":\n",
    "            doc = fitz.open(file_path)\n",
    "            return \"\\n\".join(page.get_text() for page in doc)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {suffix}\")\n",
    "    \n",
    "\n",
    "    #Split text into overlapping chunks.\n",
    "    def chunk_text(self, text: str) -> List[str]:\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        chunks = []\n",
    "        step = self.chunk_size - self.chunk_overlap\n",
    "        for i in range(0, len(tokens), step):\n",
    "            chunk = tokens[i:i + self.chunk_size]\n",
    "            chunks.append(\" \".join(chunk))\n",
    "        return chunks\n",
    "    \n",
    "    # Add documents to the retriever.\n",
    "    # Load, chunk, embed, and index documents.\n",
    "    def add_documents(self, file_paths: List[str]):\n",
    "        \n",
    "        all_chunks = []\n",
    "        for path in file_paths:\n",
    "            raw_text = self.load_document(path)\n",
    "            chunks = self.chunk_text(raw_text)\n",
    "            all_chunks.extend(chunks)\n",
    "\n",
    "        self.documents = all_chunks\n",
    "\n",
    "        # Create embeddings and build FAISS index\n",
    "        embeddings = self.model.encode(all_chunks, show_progress_bar=True)\n",
    "        dim = embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatL2(dim)\n",
    "        self.index.add(embeddings)\n",
    "\n",
    "    # Query the indexed documents and return top_k most relevant chunks.\n",
    "    def query(self, query_text: str, top_k: int = 3) -> List[str]:\n",
    "        \n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Index not initialized. Add documents first.\")\n",
    "        query_emb = self.model.encode([query_text])\n",
    "        distances, indices = self.index.search(query_emb, top_k)\n",
    "        return [self.documents[i] for i in indices[0]]\n",
    "    \n",
    "    #Save documents list and FAISS index to disk.\n",
    "    def save(self, folder: str):\n",
    "        \n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        with open(os.path.join(folder, \"documents.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(self.documents, f)\n",
    "        faiss.write_index(self.index, os.path.join(folder, \"index.faiss\"))\n",
    "\n",
    "\n",
    "    #Load documents list and FAISS index from disk.\n",
    "\n",
    "    def load(self, folder: str):\n",
    "        \"\"\"\n",
    "        Load documents list and FAISS index from disk.\n",
    "\n",
    "        Args:\n",
    "            folder (str): Directory path where files are saved.\n",
    "        \"\"\"\n",
    "        with open(os.path.join(folder, \"documents.pkl\"), \"rb\") as f:\n",
    "            self.documents = pickle.load(f)\n",
    "        self.index = faiss.read_index(os.path.join(folder, \"index.faiss\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = Retriever(chunk_size=100, chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n"
     ]
    }
   ],
   "source": [
    "retriever.add_documents([\"Rotkäppchen.pdf\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      "Rotkäppchen Es war einmal ein kleines süßes Mädchen, das hatte jedermann lieb, der sie nur ansah, am allerliebsten aber ihre Großmutter, die wusste gar nicht, was sie alles dem Kinde geben sollte. Einmal schenkte sie ihm ein Käppchen von rotem Samt, und weil ihm das so wohl stand, und es nichts anders mehr tragen wollte, hieß es nur das Rotkäppchen. Eines Tages sprach seine Mutter zu ihm: \"Komm, Rotkäppchen, da hast du ein Stück Kuchen und eine Flasche Wein, bring das der Großmutter hinaus; sie ist krank und schwach und wird sich daran laben. Mach dich auf, bevor es heiß\n",
      "\n",
      "Result 2:\n",
      "gab ihr die Hand darauf. Die Großmutter aber wohnte draußen im Wald, eine halbe Stunde vom Dorf. Wie nun Rotkäppchen in den Wald kam, begegnete ihm der Wolf. Rotkäppchen aber wusste nicht, was das für ein böses Tier war, und fürchtete sich nicht vor ihm. \"Guten Tag, Rotkäppchen!\" sprach er. \"Schönen Dank, Wolf!\" - \"Wo hinaus so früh, Rotkäppchen?\" - \"Zur Großmutter.\" - \"Was trägst du unter der Schürze?\" - \"Kuchen und Wein. Gestern haben wir gebacken, da soll sich die kranke und schwache Großmutter etwas zugut tun und sich damit stärken.\" - \"Rotkäppchen, wo wohnt deine Großmutter?\" - \"Noch\n",
      "\n",
      "Result 3:\n",
      "die kranke und schwache Großmutter etwas zugut tun und sich damit stärken.\" - \"Rotkäppchen, wo wohnt deine Großmutter?\" - \"Noch eine gute Viertelstunde weiter im Wald, unter den drei großen Eichbäumen, da steht ihr Haus, unten sind die Nusshecken, das wirst du ja wissen,\" sagte Rotkäppchen. Der Wolf dachte bei sich: Das junge, zarte Ding, das ist ein fetter Bissen, der wird noch besser schmecken als die Alte. Du musst es listig anfangen, damit du beide schnappst. Da ging er ein Weilchen neben Rotkäppchen her, dann sprach er: \"Rotkäppchen, sieh einmal die schönen Blumen, die ringsumher stehen. Warum guckst du\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Ist Rotkäppchen ein Mädchen?\" \n",
    "results = retriever.query(query, top_k=3)\n",
    "\n",
    "for i, chunk in enumerate(results):\n",
    "    print(f\"Result {i+1}:\\n{chunk}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
